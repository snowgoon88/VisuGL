* Dependance
** OpenGL
** GLFW - Window with OpenGL
** FTGL - Draw text with OpenGL and FreeType2
** GSL - Gnu Scientific Library
** BOOST - Boost::program_options
* DONE [2/2] Fenêtre GLFW version 3 
** DONE CMake pour GLFW
** DONE C++ pour GLFW
* [5/6] Afficher Graphe
** TODO Afficher Grille
** DONE [4/4] Afficher Axes avec ticks et range
*** DONE Label
 => Attention, le fait de scaler par rapport au glOrtho fait que les lettres ne sont pas jolie.
*** DONE Major Ticks
*** DONE Minor Ticks
*** DONE Ticks Label
** DONE Afficher text avec FTGL
 =>  FTGLTextureFont
** DONE Taille du texte en fonction de la taille de la fenêtre (ratio).
** DONE Position du texte en fonction de la taille de la fenètre (ratio)
** DONE Afficher Courbe

* DONE [1/1] Constructeurs, Copy, Move
** DONE tester Copy pour Layer
* TODO [8/10] BICA -HMM
** Séquence
wbuild/xp/xp-003-hmm --create_hmm "AAAAF" --save_hmm my_hmm
wbuild/xp/xp-003-hmm -m my_hmm --length_traj 1000 --save_traj my_traj
wbuild/xp/xp-003-hmm -m my_hmm -t my_traj --res_size 10 --save_esn my_esn 
wbuild/xp/xp-003-hmm -m my_hmm -t my_traj -e my_esn --noise_length 500 --save_noise my_noise
wbuild/xp/xp-003-hmm -m my_hmm -t my_traj -e my_esn -o my_res00 -g
wbuild/xp/xp-003-hmm -m my_hmm -t my_traj -e my_esn -n my_noise.dat -o my_res01 -g

** hmm.hpp (et input.hpp)
HMM est une paire de deux fontion
- une qui donne le prochain état          (int) -> int
  typedef std::function<int(int)>    T;
- une qui donne la prochaine observation  (int) -> double
  typedef std::function<double(int)> O;
*** peut définir des HMM avec string : make( string )
std::string("ABCD"),           // periodic
"AB*D",                        // periodic, mais avec *=uniform obs
"+ ABC & DEF",                 // suite de 2 HMM
"! .05 ABCD",                  // bruit gaussien sur obs
"+ ! .05 ABC & DEF",           // suite gaussien puis deterministe
"| .05 ABC .05 DEF",           // alterne entre deux deterministes
"| .05 ! .05 ABCD .01 *",      // alterne entre 1) Gaussien 2) random O
"| 0.03 ! 0.05 AAAAAAAAAAF 0.1 | 0.5 A 0.5 F"
                               // alterne 1) AAAAAAAAAAAF gaussien
                               //         2) soit A, soit F
** DONE test-hmm.cpp
** DONE [3/3] Apprendre (o)_t -> o_{t+1} avec réservoir
*** DONE générer trajectoire HMM
*** DONE initialiser différemment ESN (Szita)
*** DONE xp-003-HMM
** DONE <2016-07-21 Thu> 51F, 7AF, ABCDEF
l_hmm = ['ABCDEFEDCB','AAAAAF','AAAAAAAF']
    l_hmm_names = ['ABCDEF', '5AF','7AF']
    l_traj_size = [100,500,1000]
    l_esn_size = [10,20]
    l_leak = [0.1,0.5,0.9]
    l_forward = [ True, False ]
    l_noise_length = [0,500]
    l_regul = [0.01, 0.1,1.0,10.0]
    l_test_length = [10,50]
    
    nb_traj    = 2       ## how many instances of each traj config
    nb_esn     = 2       ## how many instances of each esn config
    nb_noise   = 2
    nb_repeat  = 1       ## no need to repeat : deterministic learning
    nb_start   = 0       ## start numbering files with
    generate_hmm  = True     ## need to generate hmm
    generate_traj = True     ## need to generate traj
    generate_esn  = True     ## need to generate esn
    generate_noise= True     ## need to generate oise
    learn         = True     ## learn
    save_learned  = True     ## save learned ESN 
*** R : df_5AF, df_7AF et df_ABCDE
*** Résultats
**** DONE 5AF (ltest=50)
- 10 meilleurs et 10 pires INSTANCES comme plot dans Pic
- Chercher si avec ltraj=100 et l_esn=10 on peut avoir bon résultats => OUI
- paramètre regul semble important (0.01, voire 0.1 mais pas plus)
- 10 meilleurs INSTANCES dans Pic
- cela se confirme en moyenne, mais il y a aussi avec regul=10
     ltraj lesn leak regul fw lnoise ltest type prec_err      mse_err
1086  1000   20  0.1  0.01  Y    500    50 test        1 1.765566e-07
1140  1000   20  0.1 10.00  Y    500    50 test    1.000 8.434848e-04
1146  1000   20  0.5 10.00  Y    500    50 test    1.000 8.434848e-04
1152  1000   20  0.9 10.00  Y    500    50 test    1.000 8.434848e-04
996   1000   20  0.1 10.00  Y      0    50 test    1.000 1.360339e-03
1002  1000   20  0.5 10.00  Y      0    50 test    1.000 1.360339e-03
1008  1000   20  0.9 10.00  Y      0    50 test    1.000 1.360339e-03
**** DONE 7AF (ltest=50)
- même constations que pour 5AF
- On essaie aussi avec ltraj=100 et lesn=10
=> Cela n'est plus suffisant pour apprendre !! best_prec = 0.76, et les meilleurs sont SANS forward ( moins de poids ??)
- 10 meilleurs INSTANCES dans Pic
- En moyenne, bcp moins de contextes donnent un bon résultats.
=> Dans l'ordre
     ltraj lesn leak regul fw lnoise ltest type prec_err      mse_err
1086  1000   20  0.1  0.01  Y    500    50 test    1.000 1.818500e-05
1092  1000   20  0.5  0.01  Y    500    50 test    1.000 1.818500e-05
1098  1000   20  0.9  0.01  Y    500    50 test    1.000 1.818500e-05
942   1000   20  0.1  0.01  Y      0    50 test    1.000 6.270271e-05
948   1000   20  0.5  0.01  Y      0    50 test    1.000 6.270271e-05
954   1000   20  0.9  0.01  Y      0    50 test    1.000 6.270271e-05
1085   500   20  0.1  0.01  Y    500    50 test    1.000 6.834068e-05
1091   500   20  0.5  0.01  Y    500    50 test    1.000 6.834068e-05
1097   500   20  0.9  0.01  Y    500    50 test    1.000 6.834068e-05
941    500   20  0.1  0.01  Y      0    50 test    1.000 2.338607e-04
947    500   20  0.5  0.01  Y      0    50 test    1.000 2.338607e-04
953    500   20  0.9  0.01  Y      0    50 test    1.000 2.338607e-04
1104  1000   20  0.1  0.10  Y    500    50 test    0.986 4.611783e-04
1110  1000   20  0.5  0.10  Y    500    50 test    0.986 4.611783e-04
1116  1000   20  0.9  0.10  Y    500    50 test    0.986 4.611783e-04
1014  1000   20  0.1  0.01  N    500    50 test    0.972 5.631581e-04
1020  1000   20  0.5  0.01  N    500    50 test    0.972 5.631581e-04
1026  1000   20  0.9  0.01  N    500    50 test    0.972 5.631581e-04
1103   500   20  0.1  0.10  Y    500    50 test    0.952 1.386075e-03
1109   500   20  0.5  0.10  Y    500    50 test    0.952 1.386075e-03
1115   500   20  0.9  0.10  Y    500    50 test    0.952 1.386075e-03
1013   500   20  0.1  0.01  N    500    50 test    0.952 1.495854e-03
1019   500   20  0.5  0.01  N    500    50 test    0.952 1.495854e-03
1025   500   20  0.9  0.01  N    500    50 test    0.952 1.495854e-03
869    500   20  0.1  0.01  N      0    50 test    0.952 4.949468e-03
875    500   20  0.5  0.01  N      0    50 test    0.952 4.949468e-03
**** DONE ABCDEF
- Bcp de bon résultats (médiane err_prec = 1)
- Les meilleurs sont avec ltraj=1000,lesn=20,forward=Y
- 10 best et 10 worst dans Pic
- En moyenn
     ltraj lesn leak regul fw lnoise ltest type prec_err      mse_err
1086  1000   20  0.1  0.01  Y    500    50 test        1 9.801589e-07
1092  1000   20  0.5  0.01  Y    500    50 test        1 9.801589e-07
1098  1000   20  0.9  0.01  Y    500    50 test        1 9.801589e-07
942   1000   20  0.1  0.01  Y      0    50 test        1 1.388668e-06
948   1000   20  0.5  0.01  Y      0    50 test        1 1.388668e-06
954   1000   20  0.9  0.01  Y      0    50 test        1 1.388668e-06
1014  1000   20  0.1  0.01  N    500    50 test        1 2.482809e-06
1020  1000   20  0.5  0.01  N    500    50 test        1 2.482809e-06
1026  1000   20  0.9  0.01  N    500    50 test        1 2.482809e-06
1085   500   20  0.1  0.01  Y    500    50 test        1 4.170694e-06
1091   500   20  0.5  0.01  Y    500    50 test        1 4.170694e-06
1097   500   20  0.9  0.01  Y    500    50 test        1 4.170694e-06
941    500   20  0.1  0.01  Y      0    50 test        1 7.021464e-06
947    500   20  0.5  0.01  Y      0    50 test        1 7.021464e-06
953    500   20  0.9  0.01  Y      0    50 test        1 7.021464e-06
870   1000   20  0.1  0.01  N      0    50 test        1 7.797301e-06
876   1000   20  0.5  0.01  N      0    50 test        1 7.797301e-06
882   1000   20  0.9  0.01  N      0    50 test        1 7.797301e-06
1013   500   20  0.1  0.01  N    500    50 test        1 9.676237e-06
- Mais ca marrche aussi avec ltraj=100 et lesn=20
084   100   20  0.1  0.01  Y    500    50 test     1.00 2.217160e-04
012   100   20  0.1  0.01  N    500    50 test     1.00 3.010283e-04
- Un peu moins avec lesn=10
     ltraj lesn leak regul fw lnoise ltest type prec_err     mse_err
1081   100   10  0.1  0.01  Y    500    50 test     0.98 0.001469465
1087   100   10  0.5  0.01  Y    500    50 test     0.98 0.001469465
1093   100   10  0.9  0.01  Y    500    50 test     0.98 0.001469465
1009   100   10  0.1  0.01  N    500    50 test     0.94 0.002306352
1015   100   10  0.5  0.01  N    500    50 test     0.94 0.002306352
1021   100   10  0.9  0.01  N    500    50 test     0.94 0.002306352
1099   100   10  0.1  0.10  Y    500    50 test     0.92 0.003070745
1105   100   10  0.5  0.10  Y    500    50 test     0.92 0.003070745
1111   100   10  0.9  0.10  Y    500    50 test     0.92 0.003070745
1027   100   10  0.1  0.10  N    500    50 test     0.89 0.003371991
1033   100   10  0.5  0.10  N    500    50 test     0.89 0.003371991
1039   100   10  0.9  0.10  N    500    50 test     0.89 0.003371991
937    100   10  0.1  0.01  Y      0    50 test     0.80 0.005466400
943    100   10  0.5  0.01  Y      0    50 test     0.80 0.005466400
949    100   10  0.9  0.01  Y      0    50 test     0.80 0.005466400
865    100   10  0.1  0.01  N      0    50 test     0.78 0.005447919
871    100   10  0.5  0.01  N      0    50 test     0.78 0.005447919
877    100   10  0.9  0.01  N      0    50 test     0.78 0.005447919
=> du bruit, faible regul
- 10 plot de  
     ltraj lesn leak regul fw lnoise ltest type prec_err     mse_err
1081   100   10  0.1  0.01  Y    500    50 test     0.98 0.001469465
** DONE <2016-09-20 Tue> p05ABCB
l_hmm = ['! .05 ABCB']
    l_hmm_names = ['p05ABCB']
    l_traj_size = [100,500,1000]
    l_esn_size = [10,20]
    l_leak = [0.1,0.5,0.9]
    l_forward = [ True, False ]
    l_noise_length = [0,500]
    l_regul = [0.01, 0.1,1.0,10.0]
    l_test_length = [50]
    
    nb_traj    = 2       ## how many instances of each traj config
    nb_esn     = 5       ## how many instances of each esn config
    nb_noise   = 2
    nb_repeat  = 1       ## no need to repeat : deterministic learning
    nb_start   = 0       ## start numbering files with
    generate_hmm  = True     ## need to generate hmm
    generate_traj = True     ## need to generate traj
    generate_esn  = False     ## need to generate esn
    generate_noise= False     ## need to generate oise
    learn         = True     ## learn
    save_learned  = True     ## save learned ESN 
*** R : df_p05ABCB.rdata
*** DONE Résultats
Comme les précédents
**** p05ABCB (ltest=50)
VOIR r_scripts/session_p05ABCB_161124
***** DONE 10 meilleures et pires INSTANCES comme plot dans Pic
***** DONE Résultats en moyenne (25 meilleurs)
les traj les plus longues et les réseaux les plus gros; et plutôt avec FORWARD
    ltraj lesn leak regul fw lnoise ltest prec_err     mse_err
90   1000   20  0.9  0.01  Y      0    50    0.948 0.002880622
288  1000   20  0.9 10.00  Y    500    50    0.948 0.002977148
87   1000   10  0.9  0.01  Y      0    50    0.946 0.003218926
108  1000   20  0.9  0.10  Y      0    50    0.944 0.002904071
126  1000   20  0.9  1.00  Y      0    50    0.944 0.002940892
144  1000   20  0.9 10.00  Y      0    50    0.944 0.003035688
286   100   20  0.9 10.00  Y    500    50    0.944 0.003418050
105  1000   10  0.9  0.10  Y      0    50    0.942 0.003272301
252  1000   20  0.9  0.10  Y    500    50    0.942 0.002881387
231  1000   10  0.9  0.01  Y    500    50    0.941 0.003165217
196   100   20  0.9  1.00  N    500    50    0.940 0.003249341
234  1000   20  0.9  0.01  Y    500    50    0.940 0.002843301
270  1000   20  0.9  1.00  Y    500    50    0.939 0.002929746
162  1000   20  0.9  0.01  N    500    50    0.939 0.002877010
180  1000   20  0.9  0.10  N    500    50    0.938 0.002911614
198  1000   20  0.9  1.00  N    500    50    0.937 0.002984891
36   1000   20  0.9  0.10  N      0    50    0.936 0.002986612
18   1000   20  0.9  0.01  N      0    50    0.936 0.002936599
249  1000   10  0.9  0.10  Y    500    50    0.934 0.003203526
244   100   20  0.5  0.10  Y    500    50    0.932 0.003396738
267  1000   10  0.9  1.00  Y    500    50    0.932 0.003300518
214   100   20  0.9 10.00  N    500    50    0.931 0.003496479
123  1000   10  0.9  1.00  Y      0    50    0.930 0.003473488
54   1000   20  0.9  1.00  N      0    50    0.928 0.003145100
216  1000   20  0.9 10.00  N    500    50    0.928 0.003234090

** DONE <2016-11-30 Wed> p05ABCDEFEDCB
l_hmm = ['! .05 ABCDEFEDCB']
    l_hmm_names = ['p05ABCDEFEDCB']
    l_traj_size = [100,500,1000,2000]
    l_esn_size = [10,20]
    l_leak = [0.1,0.5,0.9]
    l_forward = [ True, False ]
    l_noise_length = [0,500]
    l_regul = [0.01, 0.1,1.0,10.0]
    l_test_length = [50]
    
    nb_traj    = 2       ## how many instances of each traj config
    nb_esn     = 5       ## how many instances of each esn config
    nb_noise   = 2
    nb_repeat  = 1       ## no need to repeat : deterministic learning
    nb_start   = 0       ## start numbering files with
    generate_hmm  = True     ## need to generate hmm
    generate_traj = True     ## need to generate traj
    generate_esn  = False     ## need to generate esn
    generate_noise= False     ## need to generate oise
    learn         = True     ## learn
    save_learned  = True     ## save learned ESN 
*** R : df_p05ABCDEFEDCB.rdata
*** DONE Résultats
***** 10 meilleures et pires INSTANCES comme plot dans Pic
***** Résultats en moyenne (30 meilleurs)
Il faut un "gros" esn, avec leaky assez haut et du bruit (pour les premiers)
    ltraj lesn leak regul fw lnoise ltest prec_err     mse_err
240  2000   20  0.9  0.10  N    500    50    0.901 0.003659408
311  1000   20  0.9  0.01  Y    500    50    0.893 0.004065725
216  2000   20  0.9  0.01  N    500    50    0.889 0.003634816
335  1000   20  0.9  0.10  Y    500    50    0.889 0.004112659
111  1000   20  0.5  0.01  Y      0    50    0.884 0.004007151
119  1000   20  0.9  0.01  Y      0    50    0.884 0.004109359
214   500   20  0.9  0.01  N    500    50    0.884 0.004206693
312  2000   20  0.9  0.01  Y    500    50    0.883 0.003969087
143  1000   20  0.9  0.10  Y      0    50    0.882 0.004215824
215  1000   20  0.9  0.01  N    500    50    0.882 0.004169597
264  2000   20  0.9  1.00  N    500    50    0.882 0.003878625
303  1000   20  0.5  0.01  Y    500    50    0.879 0.004102738
110   500   20  0.5  0.01  Y      0    50    0.876 0.004265230
238   500   20  0.9  0.10  N    500    50    0.876 0.004390473
120  2000   20  0.9  0.01  Y      0    50    0.874 0.004048605
302   500   20  0.5  0.01  Y    500    50    0.873 0.004306892
24   2000   20  0.9  0.01  N      0    50    0.872 0.003900116
336  2000   20  0.9  0.10  Y    500    50    0.871 0.003998231
48   2000   20  0.9  0.10  N      0    50    0.870 0.003973434
206   500   20  0.5  0.01  N    500    50    0.870 0.004636049
359  1000   20  0.9  1.00  Y    500    50    0.868 0.004532948
239  1000   20  0.9  0.10  N    500    50    0.866 0.004327435
22    500   20  0.9  0.01  N      0    50    0.864 0.004619682
263  1000   20  0.9  1.00  N    500    50    0.861 0.004820195
328  2000   20  0.5  0.10  Y    500    50    0.861 0.004263758
23   1000   20  0.9  0.01  N      0    50    0.860 0.004486881
72   2000   20  0.9  1.00  N      0    50    0.860 0.004407411
288  2000   20  0.9 10.00  N    500    50    0.860 0.004814376
304  2000   20  0.5  0.01  Y    500    50    0.860 0.004238826
112  2000   20  0.5  0.01  Y      0    50    0.858 0.004287934
** DONE <2016-12-20 Tue> p05AAAAAF
l_hmm = ['! .05 AAAAAF']
    l_hmm_names = ['p05AAAAAF']
    l_traj_size = [100,500,1000,2000]
    l_esn_size = [10,20]
    l_leak = [0.1,0.5,0.9]
    l_forward = [ True, False ]
    l_noise_length = [0,500]
    l_regul = [0.01, 0.1,1.0,10.0]
    l_test_length = [50]
    
    nb_traj    = 2       ## how many instances of each traj config
    nb_esn     = 5       ## how many instances of each esn config
    nb_noise   = 2
    nb_repeat  = 1       ## no need to repeat : deterministic learning
    nb_start   = 0       ## start numbering files with
    generate_hmm  = True     ## need to generate hmm
    generate_traj = True     ## need to generate traj
    generate_esn  = False     ## need to generate esn
    generate_noise= False     ## need to generate oise
    learn         = True     ## learn
    save_learned  = True     ## save learned ESN 
*** R : df_p05AAAAAF.rdata
*** Résultats
***** 10 meilleures et pires INSTANCES comme plot dans Pic
***** Résultats en moyenne (30 meilleurs)
ltraj lesn leak regul fw lnoise ltest prec_err     mse_err
311  1000   20  0.9  0.01  Y    500    50    0.891 0.004251172
336  2000   20  0.9  0.10  Y    500    50    0.881 0.004789378
120  2000   20  0.9  0.01  Y      0    50    0.872 0.005041548
335  1000   20  0.9  0.10  Y    500    50    0.872 0.004500103
312  2000   20  0.9  0.01  Y    500    50    0.871 0.004743521
360  2000   20  0.9  1.00  Y    500    50    0.870 0.005178340
144  2000   20  0.9  0.10  Y      0    50    0.868 0.005074269
119  1000   20  0.9  0.01  Y      0    50    0.864 0.004855129
143  1000   20  0.9  0.10  Y      0    50    0.864 0.005089554
216  2000   20  0.9  0.01  N    500    50    0.851 0.005018916
168  2000   20  0.9  1.00  Y      0    50    0.848 0.005390173
215  1000   20  0.9  0.01  N    500    50    0.834 0.005344498
240  2000   20  0.9  0.10  N    500    50    0.829 0.005556742
239  1000   20  0.9  0.10  N    500    50    0.824 0.006197300
310   500   20  0.9  0.01  Y    500    50    0.820 0.005553121
24   2000   20  0.9  0.01  N      0    50    0.818 0.006147999
48   2000   20  0.9  0.10  N      0    50    0.812 0.006572256
359  1000   20  0.9  1.00  Y    500    50    0.811 0.005753680
334   500   20  0.9  0.10  Y    500    50    0.803 0.005757001
167  1000   20  0.9  1.00  Y      0    50    0.796 0.006277051
384  2000   20  0.9 10.00  Y    500    50    0.788 0.007029268
192  2000   20  0.9 10.00  Y      0    50    0.786 0.007232797
118   500   20  0.9  0.01  Y      0    50    0.782 0.006405694
142   500   20  0.9  0.10  Y      0    50    0.776 0.006555891
264  2000   20  0.9  1.00  N    500    50    0.776 0.007611161
** TODO p05AAAAAAAF
** TODO autres chaines (12 A?)
** DONE [0/1] Visualization with R
*** TODO Tester si autres scripts marchent.
** DONE visualization on-line
* TODO Rec-SOM
* [/] Data
** DONE McKey-Glass
*** DONE Serialiser avec gaml::make-output_iterator ?
=> son propre reader/writer
* TODO [3/4] Reservoir Computing
** DONE Serialiser
** DONE Learn
Mise en place de Ridge Regression
** DONE [2/4] Reservoir
*** DONE GSL => valeurs propres complexes
Comment c'est fait dans Matlab : Magnitude du complexe
*** DONE Copie de la matrice avant les calculs
*** TODO C++ Matrix
BLAZE : https://code.google.com/p/blaze-lib/
Eigen
*** TODO Iterator ??
http://stackoverflow.com/questions/7758580/writing-your-own-stl-container/7759622#7759622

** TODO Cherche le meilleur Ridge Coef
* TODO Data en séquence
* TODO [7/10] XP POMDP 
J'ai l'impression qu'essayer d'apprendre les transitions ne marche que dans des cas quasi déterministes, non? Dans ce cas, faudrait plutôt essayer de voir si on peut pas apprendre V(s) ou V(o) ??
** DONE ajout d'un jsonreader pour transition
** DONE Générer et sauvegarder une trajectoire de transitions
** DONE Cheese maze labyrinthe
Voir début danns xp/xp-002-cheese-maze
** DONE Memory Leak
** DONE Sauvegarder les résultats
** DONE pour apprendre il faut traj+pomdp (nb obs, etc)
** DONE [4/4] Comprendre le format de fichier de sortie -> R
*** GEN pomdp cheese
wbuild/xp/xp-002-cheese -p 0.9 -l 1 -f cheese_maze_0.9_1
*** GEN trajectoire
wbuild/xp/xp-001-pomdp -p data_xp/cheese_maze_0.9_1.json --gene_traj data_xp/traj_1000 --traj_length 1000
*** GEN esn
wbuild/xp/xp-001-pomdp -p data_xp/cheese_maze_0.9_1.json --gene_esn esn_50_1_0.99_0.1 --res_size 50
*** GEN noise
wbuild/xp/xp-001-pomdp -p data_xp/cheese_maze_0.9_1.json --gene_noise noise_1000_0.1 --length_noise 1000
*** DONE LEARN
*** DONE LEARN Paremeters
wbuild/xp/xp-001-pomdp -p data_xp/cheese_maze_0.9_1.json -t data_xp/traj_1000.data -e data_xp/esn_50_1_0.99_0.1.json --regul 10.0 -o data_xp/result_10.data
*** DONE LEARN results with comment and header
*** DONE [Q] : biais dans réservoir ? => OUI
d'après [Lukosevicius12]
** [11/13] TODO Scripts pour tester influence des paramètres
## calculer taux erreur pour chaque fichier
df.sum <- make_df_pomdp( "data_xp" )
## sauvegarde
write.table( df.sum, file="data_xp/df_sum.rdata", row.names=FALSE, sep="\t")
## Fait en plusieurs lots qu'on charge ensuite
df <- rbind( df.sum.500, df.sum.1000, df.sum.2000, df.sum.10000 )
*** TODO PT Max id : ltraj lesn leak regul
voir dans [[bac_learnpomdp.R]]
## Faire les moyenne
attach(df.sum)
df.mean <- aggregate(df.sum[,c("rate_le","mse_le")], by=list(ltraj,lesn,leak,regul,ltest,type), FUN=mean)
detach( df.sum )
## Remplacer "Groupe.1" par son 'vrai nom
names(df.mean)[1:6] <- c("ltraj","lesn","leak","regul","ltest","type")
## trouver le max de rate_le
attach( df.mean )
which.max( rate_le )
## etc
## Ordonner le dataframe en fonction de rate_le
df.maxrate <- df.mean[ order(-rate_le),]
## On peut ensuite afficher, puis imprimer, en cliquant dessus.

*** DONE Visualier autour d'un point
res <- mk_query( list(1000,100,0.1,0.1,10,"test"))
subd <- get_filenames( df.sum, list(1000,100,0.1,0.1,10,"test"))
lp <- plot_traj_esn( subd, esn=6, str.title=res\[[3 ]\] )
## Ce qui se fait aussi avec 
p <- look_traj( df.sum, list(500,100,0.5,1,100,"test"), 2)
subd.test05 <- p[[1]]
str.test05 <- p[[2]]
pt.test05 <- p[[3]]
plot_traj_esn( subd.test05, esn=6, str.title=str.test05)
plot_traj_esn( subd.test05, esn=9, str.title=str.test05)

**** <2016-05-24 Tue>
Sauvegarde de l'environnement R autour de data_xp dans "env_data_xp.R"

*** DONE Tester sur traj test !!
*** DONE Afficher variation autour PT Max dans chacune des 4 dimensions (3 fixe)
*** DONE R-Script : générer les noms des fichiers results
*** DONE R-Script : utiliser 'by' pour faire des moyennes, des variances
*** DONE Regarder Variabilité Traj et Output dans 100  200  0.5   0.1
*** DONE Reprendre script python
ATTENTION : il faut générer un ESN par run de l'expérience.
**** subprocess.Popen 
     avec Popen.poll() et Popen.wait()
*** DONE taille Res
*** DONE Leaking rate
*** DONE regul
*** noise
*** noise length
*** DONE traj size
*** TODO Sauvegarder les points pt.test* importants

** TODO [1/4] apprendre (0+A) -> .
*** TODO pour apprendre, il faut transition ->O
*** DONE pour apprendre, il faut transition ->S (pour voir)
**** <2016-04-20 Wed>
l_traj_size = [500,1000,2000,10000]
l_esn_size = [10,50,100]
l_regul = [0.01, 0.1, 1.0, 10.0]
l_leak = [0.1,0.5,0.9]
l_test_length = [10,100,1000]
nb_traj    = 5       ## how many instances of each traj config
nb_esn     = 10      ## how many instances of each esn config
nb_repeat  = 1       ## no need to repeat : deterministic learning
nb_start   = 0       ## start numbering files with
generate   = True    ## need to generate traj,esn
data_xp/result_traj_size_esn_size_leak_regul_Eesn_Ttraj.data_nb_test/learn
*** TODO pour apprendre, il faut transition ->V(S) (pour voir)
**** TODO <2016-05-23 Mon>
l_traj_size = [500,1000,2000,10000]
    l_esn_size = [10,50,100]
    l_regul = [0.01, 0.1, 1.0, 10.0]
    l_leak = [0.1,0.5,0.9]
    l_test_length = [10,100,400]
    
    nb_traj    = 5       ## how many instances of each traj config
    nb_esn     = 2      ## how many instances of each esn config
    nb_repeat  = 1       ## no need to repeat : deterministic learning
    nb_start   = 0       ## start numbering files with
    generate_traj = False    ## need to generate traj
    generate_esn  = True     ## need to generate traj
    learn         = True    ## learn
*** TODO pour apprendre, il faut tansition ->V(O) (pour voir)
** TODO [0/3] apprendre A x (O -> .) 
*** TODO pour apprendre, il faut transition ->O
*** TODO pour apprendre, il faut transition ->S (pour voir)
*** TODO pour apprendre, il faut transition ->V(S) (pour voir)
* [0/1] Utils
** TODO Un seul namespace dans utils, la différence se fera par l'objet que l'on veut striser
* [3/3] Check Ridge Regression
https://onlinecourses.science.psu.edu/stat857/node/155
http://www.astrostatistics.psu.edu/su07/R/html/MASS/html/lm.ridge.html
** DONE RidgeReg with GIVEN regul param
** DONE Compare with R
** DONE Ne pas accorder de pénalité pour le poids associé à 'intercept' dans RR
* [7/10] DSOM
** TODO [/] plot DSOM in R
*** $neurons : list
*** $neurons[[1]]$pos : list of 2 values
*** $neurons [ [1] ]$link : list of neigbors id
** DONE plot densities with R
** DONE normal density
** DONE ring density
** DONE to double
** DONE Eigen Random ?? => entre -1 et 1 ??
** TODO Eigen Random Seed ??
=> use srand if needed
** [2/2] Serialize
*** DONE Write to JSON
*** DONE Read from JSON
** TODO Copy operator/assignment => copie de l_link et l_neighbors
** DONE quelles fonctions utiles ?
** DONE need regularly put pos neurons
* [5/7] REC_DSOM
** DONE recurrent neuron [dsom/r_neuron.hpp]
** DONE recurrent network [dsom/r_network.hpp]
** DONE Visualization de la récurrence
** DONE Ajouter d'autres graphes
** DONE Générer traj qui soit multiples de cycles. Et apprendre par cycle.
*** P/M : augmenter/diminuer le step
*** SPACE : run ON/OFF
*** S : step
*** V : verbose ON/OFF
** TODO Info sur ce qui est appris
** TODO Expé pour bons paramètres
*** Génération de traj
build/xp/xp-003-hmm -m data_rdsom/hmm_p05AAAAAF.json --length_traj 600 --save_traj data_rdsom/traj_p05AAAAAF_600_n000.data
*** XP
*** <2017-03-03 Fri>
wbuild/xp/xp-004-rdsom -t data_rdsom/traj_p05AAAAAF_600_n000.data -d data_rdsom/rdsom_50.json -g --queue_size 6
| beta | sig_i | sig_r | sig_c | eps | ela |
|------+-------+-------+-------+-----+-----|
| 0.5  | 0.1   | 0.1   | 0.1   | 0.1 | 0.2 |  que quelques neur->weigts apprennent... Plus large pour neur-r_weight
15470 itérations
RNeuron Neuron [9] at (9, ) w=0.11336 =(0.18, ) rw=0.485828 
    INPUT: dnorm= 0.162916; hn=2.31854e-12 => delta=-7.18282e-15
    REC  : dnorm= 0.273268; hn=0.486059 =>  delta=-0.00353083
  RNeuron Neuron [10] at (10, ) w=-0.0213353 =(0.2, ) rw=0.459814 
    INPUT: dnorm= 0.0475172; hn=0.00123397 => delta=-3.25206e-07
    REC  : dnorm= 0.246526; hn=0.834972 =>  delta=-0.00493639
  RNeuron Neuron [11] at (11, ) w=-0.0307759 =(0.22, ) rw=0.453733 
    INPUT: dnorm= 0.0394291; hn=1 => delta=-0.000181462
    REC  : dnorm= 0.240275; hn=1 =>  delta=-0.00561602
  RNeuron Neuron [12] at (12, ) w=-0.0196312 =(0.24, ) rw=0.459884 
    INPUT: dnorm= 0.0489772; hn=0.00123397 => delta=-3.45497e-07
    REC  : dnorm= 0.246598; hn=0.834972 =>  delta=-0.00493927
  RNeuron Neuron [13] at (13, ) w=0.105328 =(0.26, ) rw=0.486057 
    INPUT: dnorm= 0.156035; hn=2.31854e-12 => delta=-6.58884e-15
    REC  : dnorm= 0.273503; hn=0.486059 =>  delta=-0.00353693

ela est vachement important pour gérer la portée de Hn. En gros, si on veut qu'au moins 5 neurones aient un Hn>0.1 pour une différence de poids de 0.05, il faut ela>1.3
*** <2017-03-06 Mon>
Avec ela=1.3, la couche récurrent converge dans son entier vers 0.4. Tous les neurones apprennent trop vite.
[Q] le winner global semble plus influencé par les wieghts que les r_weigts
=> peut-être que le sigma_r doit être moins sélectif ?
=> les similitudes convoluées affichées sont normalisées A L'AFFICHAGE. Mais en vrai ?
*** <2017-03-09 Thu>
Divers essais avec Yann

A marché pour ABCDEF
wbuild/xp/xp-003-hmm -m data_hmm/hmm_ABCDEF.json --length_traj 9000 --save_traj data_rdsom/traj_ABCDEF_9000.data_

wbuild/xp/xp-004-rdsom -t data_rdsom/traj_p05AAAAAF_600_n000.data -d data_rdsom/rdsom_50.json -g --queue_size 10 --dsom_ela 1.0 --dsom_sig_r 0.01 --dsom_sig_i 0.01 --dsom_beta 0.4

wbuild/xp/xp-004-rdsom -t data_rdsom/traj_p05AAAAAF_600_n000.data -d data_rdsom/rdsom_50.json -g --queue_size 10 --dsom_ela 1.0 --dsom_sig_r 0.01 --dsom_sig_i 0.01 --dsom_beta 0.9

build/xp/xp-004-rdsom -t data_rdsom/traj_p05AAAAAF_600_n000.data -d data_rdsom/rdsom_50.json -g --queue_size 10 --dsom_ela 1.3 --dsom_sig_r 0.01 --dsom_sig_i 0.01 --dsom_beta 0.6
*** <2017-03-21 Tue>
Modifier le rôle de beta
**** DONE Essayer en linéaire ? => *BAD*
merged = mu * beta + (1-beta) * nu 
pas bien car rapidement, la similarité récurrent est plus grande que la similarité des entrées. (à cause de l'exponentielle qui écrase). Du coup, on termine assez souvent sur un seul neurone en boucle. Même avec beta=0.95
**** Retour non linéaire
wbuild/xp/xp-004-rdsom -t data_rdsom/traj_p05AAAAAF_600_n000.data -d data_rdsom/rdsom_50.json -g --queue_size 10 --dsom_ela 1.2 --dsom_sig_r 0.01 --dsom_sig_i 0.01 --dsom_beta 0.1 --dsom_ela_rec 0.005 --dsom_eps 0.25
=> pas assez de distance entre les neurones actifs sur le A ?
=> cycles pas stabilisés
**** Augmenter le sigma_weight
pour avoir plus de choix dans le neurone d'entrée, que chacun puisse se spécialiser dans un rec différent ?
**** SUCCESS (à partir de 20.000)
wbuild/xp/xp-004-rdsom -t data_rdsom/traj_p05AAAAAF_6000.data -d data_rdsom/rdsom_50.json -g --queue_size 10 --dsom_ela 1.0 --dsom_sig_r 0.1 --dsom_sig_i 0.1 --dsom_beta 0.05 --dsom_ela_rec 0.01 --dsom_eps 0.25
=> après 95620
**** TODO essayer sur autres
**** TODO Critère de performance (diff de prédiction)
***** Erreur entre winner et input -> mais c'est pas suffisant
***** Erreur de prédiction => prochain et prochain input
**** TODO sauvegarder les films
**** DONE algorithme déterministe => OUI !
***** Création de Traj
***** Création de RDSOM
***** Pas de bruit dans l'apprentissage.
**** TODO Légende des courbres ??
**** TODO refaire les courbes de séquences / prédition de ESN




* [2/3] Visualisation
** DONE Ecrire du texte n'importe où 
** TODO Classe[Container] avec 2 membres "one", "two"
*** itérer sur la Classe et appliquer une fonction (ou une classe) pour récupérer l'un des deux membres
*** A tester 
** DONE Class avec un membre qui est un Container[double] => OK
   
